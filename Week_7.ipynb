{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Bipedal Robot Walk\n",
    "\n",
    "This is the midterm assignment for the Move37 course. \n",
    "\n",
    "The Goal is to make a 2D robot learn how to walk. That's a simple task, since we're basically given the code in the lectures.\n",
    "\n",
    "I'm making two substantial changes to the code:\n",
    "- adding a tanh activation (as this will naturally squeeze actions into the desired [-1, 1] range.\n",
    "- adding learning rate annealing.\n",
    "\n",
    "I also change the code, making the training part (at least to my eye) more modular and extendable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from gym import wrappers\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV = gym.make('BipedalWalker-v2')\n",
    "INIT_POP = 200\n",
    "KEEP_BEST = 15\n",
    "KEEP_NONBEST = 5\n",
    "NUM_CHILDREN = 50\n",
    "MUTATION_SIZE = 0.2\n",
    "ENV.env.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(24,) Box(4,)\n",
      "-inf inf\n",
      "[-1. -1. -1. -1.] [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(ENV.observation_space, ENV.action_space)\n",
    "print(min(ENV.observation_space.low), max(ENV.observation_space.high))\n",
    "print(ENV.action_space.low, ENV.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NS = ENV.observation_space.shape[0]\n",
    "NA = ENV.action_space.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Policy():\n",
    "    \n",
    "    '''Base class. Defines policy by a weight matrix W.'''\n",
    "    \n",
    "    def __init__(self, env = ENV, clip_reward = True):\n",
    "        self.env = env\n",
    "        self.nS = env.observation_space.shape[0]  # Num of states\n",
    "        self.nA = env.action_space.shape[0]  # Num of actions\n",
    "        self.W = np.zeros((self.nA, self.nS))  # Action dim is 0 to avoid transposes later\n",
    "        self.n = 0  # count of how many times we have played\n",
    "        self.clip_reward = clip_reward  # if true, we squeeze rewards into [-1, 1]\n",
    "    \n",
    "    def pi(self, state, W = None):\n",
    "        '''Out policy. Returns action from state.\n",
    "        \n",
    "        Note: we can optionally supply a weight matrix W, if we do so, we play\n",
    "        policy according to this supplied matrix.'''\n",
    "        \n",
    "        if W is None: W = self.W \n",
    "        return np.tanh(W @ state)\n",
    "    \n",
    "    def playPol(self, W = None, save_frames = False, save_ext = 'tmp/experiment0/'):\n",
    "        '''Plays a game from start to finish. \n",
    "        \n",
    "        Note: we can optionally supply a weight matrix W, if we do so, we play\n",
    "        policy according to this supplied matrix.'''\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if save_frames:\n",
    "            env = wrappers.Monitor(self.env, save_ext, force=True)\n",
    "        else:\n",
    "            env = self.env\n",
    "            \n",
    "        self.s = env.reset()\n",
    "            \n",
    "        for i in range(2000):\n",
    "            if save_frames: env.render()\n",
    "            self.call_before_action()\n",
    "            action = self.pi(self.s, W)\n",
    "            self.s, self.r, self.done, _ = env.step(action)\n",
    "            if self.clip_reward:\n",
    "                self.r = max(min(self.r, 1), -1)\n",
    "            reward += self.r\n",
    "            self.n += 1\n",
    "            if self.done: break\n",
    "        if save_frames: \n",
    "            env.close(); self.env.close()\n",
    "        return reward\n",
    "    \n",
    "    def call_before_action(self):\n",
    "        '''Optional method that gets executed every round before policy.'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize State\n",
    "\n",
    "The previous agent didn't normalize its state (the inputs for an action). We add normalization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class norm_Policy(Base_Policy):\n",
    "    '''Same policy as Base, except that we normalize inputs to mean 0, and std 1.'''\n",
    "    \n",
    "    def __init__(self, clip_reward = True):\n",
    "        super().__init__(clip_reward = clip_reward)\n",
    "        self.inp_mean, self.inp_var = np.zeros(self.nS), np.ones(self.nS)\n",
    "        self.mean_diff = np.zeros(self.nS)\n",
    "        \n",
    "    def call_before_action(self):\n",
    "        '''This function is called before calling our policy to get the next action.'''\n",
    "        self.update_stats()  # Updates means and std with current state value\n",
    "        if self.n >= 2: \n",
    "            self.normalize_state()  # Normalizes inputs\n",
    "\n",
    "    def update_stats(self):\n",
    "        '''Update our mean and std calculations'''\n",
    "        last_mean = self.inp_mean.copy()\n",
    "        self.inp_mean += (self.s - self.inp_mean) / max(self.n, 1)\n",
    "        self.mean_diff += (self.s - last_mean) * (self.s - self.inp_mean)\n",
    "        self.inp_var = np.clip((self.mean_diff / max(self.n, 1)), a_min = 1e-2, a_max = None)\n",
    "        \n",
    "    def normalize_state(self):\n",
    "        '''Normalize state that gets fed into our policy.'''\n",
    "        self.s = (self.s - self.inp_mean) / np.sqrt(self.inp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pol = norm_Policy()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Fitness Function\n",
    "\n",
    "We'll search for the optimal parameters of the weight matrices. For that we need a couple of ingredients:\n",
    "1. A fitness function that scores a particular weight matrix.\n",
    "1. An initial population of weight matrices.\n",
    "1. A selection mechanism.\n",
    "1. Cross-over between weight matrices (a method to combine them).\n",
    "1. Mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFitness(W, pol = norm_pol):\n",
    "    return pol.playPol(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pop = [np.random.rand(NA, NS) for _ in range(INIT_POP)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(pop, keep_best = KEEP_BEST, keep_nonbest = KEEP_NONBEST):\n",
    "    \n",
    "    fit = np.array([getFitness(w) for w in pop])\n",
    "    avg, std, maxfit = (np.mean(fit), np.std(fit), np.max(fit))\n",
    "    \n",
    "    sort_idxs = np.argsort(-fit)\n",
    "    best_idxs = list(sort_idxs[:keep_best])\n",
    "    nonbest_idxs = list(np.random.choice(len(pop), keep_nonbest, False))\n",
    "    \n",
    "    select_idxs = best_idxs+nonbest_idxs\n",
    "    \n",
    "    if len(select_idxs) == 1:\n",
    "        return pop[select_idxs[0]], avg, std, maxfit\n",
    "    \n",
    "    keep = list(itemgetter(*(select_idxs))(pop))\n",
    "        \n",
    "    return keep, avg, std, maxfit    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_over(A, B):\n",
    "    \n",
    "    assert A.shape == B.shape\n",
    "    \n",
    "    mask = np.random.randint(0, 2, size = A.shape)\n",
    "    \n",
    "    return mask*A + (1-mask)*B\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(W, mut_size = MUTATION_SIZE):\n",
    "    delta = np.random.randn(*W.shape)*2-1\n",
    "    delta = delta * mut_size\n",
    "    return W + delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Step of Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_1_step(pop, num_children = NUM_CHILDREN):\n",
    "    \n",
    "    keep, avg, std, maxfit =  selection(pop)\n",
    "    \n",
    "    children = [cross_over(*itemgetter(*np.random.choice(len(keep), 2, False))(keep)) \n",
    "                for _ in range(num_children)]\n",
    "    \n",
    "    children = [mutate(child) for child in children]\n",
    "    \n",
    "    return keep+children, avg, std, maxfit    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution over multiple generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(pop, gen):\n",
    "    for i in range(gen):\n",
    "        pop, avg, std, maxfit = evolve_1_step(pop)\n",
    "        if (i % 10 == 0) or (i == gen-1):\n",
    "            print(f'Generation {i}, average fitnes: {avg}, std: {std}, max: {maxfit}')\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0, average fitnes: -15.339396146356773, std: 23.852071868820914, max: -4.621529529635325\n",
      "Generation 10, average fitnes: -19.041435213950574, std: 33.44284080787119, max: 2.0912835829239538\n",
      "Generation 20, average fitnes: -10.70105432369192, std: 11.670403520418253, max: 2.3312709762315507\n",
      "Generation 30, average fitnes: -7.204469925928542, std: 24.34177086501751, max: 53.99564512356741\n",
      "Generation 40, average fitnes: 29.257524880620974, std: 28.76081105636012, max: 87.34558406771237\n",
      "Generation 50, average fitnes: 58.747103841192306, std: 42.43923720057645, max: 118.78397901339953\n",
      "Generation 60, average fitnes: 68.74797084057181, std: 40.87895227923038, max: 134.99773015545696\n",
      "Generation 70, average fitnes: 87.8218701216341, std: 38.90395320725124, max: 145.93027151549222\n",
      "Generation 80, average fitnes: 84.10837273155826, std: 48.30605923592216, max: 146.97929040220083\n",
      "Generation 90, average fitnes: 93.55097008263216, std: 45.10682078062708, max: 151.96563144056424\n",
      "Generation 100, average fitnes: 98.23152087900674, std: 38.621485795005704, max: 149.92184163737824\n",
      "Generation 110, average fitnes: 104.17423574442802, std: 39.2718395385763, max: 156.7770740455459\n",
      "Generation 120, average fitnes: 92.80018969156013, std: 52.4585285405461, max: 154.03078104618493\n",
      "Generation 130, average fitnes: 106.26030478371493, std: 48.15385259933712, max: 164.4003838516854\n",
      "Generation 140, average fitnes: 106.34511114632137, std: 42.383372710615475, max: 163.64865600051428\n",
      "Generation 150, average fitnes: 86.86540384900839, std: 52.24532785157889, max: 153.7857857707231\n",
      "Generation 160, average fitnes: 92.78728891160921, std: 43.45105143705393, max: 157.5714640941709\n",
      "Generation 170, average fitnes: 105.22431518366655, std: 36.33576116468543, max: 155.74940887742477\n",
      "Generation 180, average fitnes: 94.57465800425047, std: 43.81832959667104, max: 148.99277350806713\n",
      "Generation 190, average fitnes: 102.40417154468308, std: 36.94207306036508, max: 149.87913372401954\n",
      "Generation 199, average fitnes: 82.39050854498278, std: 44.48889002766051, max: 144.79739169294317\n"
     ]
    }
   ],
   "source": [
    "pop = evolve(init_pop, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best weights from population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBest(pop):\n",
    "    pop, _, _, _ = selection(pop, keep_best = 10, keep_nonbest = 0)\n",
    "    best, _, _, _ = selection(pop, keep_best = 1, keep_nonbest = 0)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pol.W = getBest(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.90901806581113"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_pol.playPol(save_frames=True, save_ext = 'bipedal_rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"bipedal_rec/openaigym.video.0.4333.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"bipedal_rec/openaigym.video.0.4333.video000000.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
