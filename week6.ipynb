{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gvpH3w8-W09R"
   },
   "source": [
    "## Learn to Play Lunar Lander\n",
    "\n",
    "I experimented with some hyper-parameter settings, but didn't get satisfactory results. So this is just the raw code, feel free to play around with different settings and see if your rewards/losses converge to a good point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "31WbqFWCW7lY"
   },
   "outputs": [],
   "source": [
    "# Install Pytorch\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "colab_type": "code",
    "id": "nWjQjAk9aAJ3",
    "outputId": "34d02538-b29e-43e3-9199-160689a93fe1"
   },
   "outputs": [],
   "source": [
    "# Needed to get the gym environment working\n",
    "!apt-get install swig3.0\n",
    "!ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "!pip3 install box2d box2d-kengz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "KjdDsscOXNRy",
    "outputId": "71184da4-d76d-4625-811f-9c450500dd23"
   },
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P445gVILW09W"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5JhN-kQeW09j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "H9KlB7NcW091",
    "outputId": "b1943335-61d6-4ce1-d05c-59378b21416e"
   },
   "outputs": [],
   "source": [
    "ENV = gym.make('LunarLander-v2')\n",
    "NS = ENV.observation_space.shape[0]  \n",
    "NA = 1\n",
    "DEFAULT_PARAMS = {'bn':10, 'bs':256, 'gamma':0.9, 'ns':NS, 'na':NA}\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rDv1s6AhW09-",
    "outputId": "4ef32326-1280-4a35-b96c-3ad33cea64ff"
   },
   "outputs": [],
   "source": [
    "print(ENV.observation_space, ENV.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "6PisYx8iW0-G",
    "outputId": "7fe0ca94-bd0a-43d6-b0f0-a7cc15bcf591"
   },
   "outputs": [],
   "source": [
    "ENV.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6ZoEn8vW0-S"
   },
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    \n",
    "    def __init__(self, env, clip_reward = False):\n",
    "        self.env = env\n",
    "        self.max_rounds = int(1e4)\n",
    "        self.clip_reward = clip_reward\n",
    "        self.n = 0  # Number of exploration rounds\n",
    "        \n",
    "    def randomAction(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def pi(self, state, explore):\n",
    "        return self.randomAction()\n",
    "    \n",
    "    def playPol(self, save_frames = False, explore = False):\n",
    "        '''Plays a game from start to finish. '''\n",
    "        \n",
    "        if explore: self.n += 1\n",
    "            \n",
    "        state = self.env.reset()\n",
    "        frames = []; reward = 0\n",
    "        self.history = deque()\n",
    "        for i in range(self.max_rounds):       \n",
    "            if save_frames: frames.append(self.env.render(mode = 'rgb_array'))\n",
    "            action = self.pi(state, explore)\n",
    "            newstate, r, done, _ = self.env.step(action)\n",
    "            self.history.append((state, action, r, done, newstate))\n",
    "            if self.clip_reward:\n",
    "                r = max(min(r, 1), -1)\n",
    "            reward += r\n",
    "            if done: break\n",
    "        if save_frames: self.env.close()\n",
    "        return (reward, frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IivvsKblW0-r"
   },
   "outputs": [],
   "source": [
    "random_guy = RandomAgent(ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WhrcjWp0bIpP",
    "outputId": "2d7994e0-0709-4d60-ca63-a4b8a5190a8d"
   },
   "outputs": [],
   "source": [
    "random_guy.playPol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "zuYK69tErAWF",
    "outputId": "c3e30374-2ffa-4e9c-e2e1-22b1ddaa33a0"
   },
   "outputs": [],
   "source": [
    "random_guy.history[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhcxoHFXeLxh"
   },
   "source": [
    "## Create Neural Net Architecture\n",
    "\n",
    "The task is pretty simple: take the inputs (8), and return the Q function (the value of each action). That is a NN with 8 inputs, 4 outputs, as many hidden layers as we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtIdybtCecha"
   },
   "outputs": [],
   "source": [
    "arch = nn.Sequential(nn.BatchNorm1d(8, affine = False),\n",
    "                    nn.Linear(8, 50), \n",
    "                    nn.LeakyReLU(inplace = True), \n",
    "                    nn.BatchNorm1d(50), \n",
    "                    nn.Linear(50, 4))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "emLJ6uKFkrjv",
    "outputId": "5b5f9f52-1d3c-4230-ed94-7e6cf64b9a0c"
   },
   "outputs": [],
   "source": [
    "state = torch.tensor(ENV.reset()[None])\n",
    "arch.eval()\n",
    "with torch.no_grad():\n",
    "    print(arch(state).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUh7nURXjP9-"
   },
   "source": [
    "## Create Deep Q Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfDufXZQlo6J"
   },
   "outputs": [],
   "source": [
    "class DQAgent(RandomAgent):\n",
    "    \n",
    "    def __init__(self, env, arch):\n",
    "        super().__init__(env)\n",
    "        self.arch = arch\n",
    "    \n",
    "    def getQ(self, state):\n",
    "        self.arch.eval()\n",
    "        with torch.no_grad():\n",
    "            inp = torch.tensor(state[None])\n",
    "            return self.arch(inp).numpy()[0]\n",
    "        \n",
    "    def getOptimalAction(self, state):\n",
    "        return np.argmax(self.getQ(state))\n",
    "    \n",
    "    def get_exp_frac(self, low_lim = 0.05, high_lim = 0.9999, n_taper = 1e-4):\n",
    "        n = self.n\n",
    "        #frac = low_lim + high_lim  / (1 + n * n_taper)\n",
    "        frac = high_lim - (high_lim - low_lim) * n * n_taper\n",
    "        self.exp_frac = max(min(frac, high_lim), low_lim)\n",
    "        \n",
    "    def playPol(self, save_frames = False, explore = False):\n",
    "        \n",
    "        self.get_exp_frac()\n",
    "        return super().playPol(save_frames, explore)\n",
    "    \n",
    "    def do_exploration(self):\n",
    "        return np.random.rand() < self.exp_frac        \n",
    "    \n",
    "    def pi(self, state, explore = False):\n",
    "        if explore and self.do_exploration():\n",
    "            return self.randomAction()\n",
    "        else:\n",
    "            return self.getOptimalAction(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fpWXhxr_mzEq",
    "outputId": "fb50ad40-1dc0-40e8-b192-36b046d3f3c9"
   },
   "outputs": [],
   "source": [
    "dqn_guy = DQAgent(ENV, arch)\n",
    "dqn_guy.playPol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9BlxaZq0ofVw"
   },
   "source": [
    "## Create a buffer of experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0SoOO73pOU4"
   },
   "outputs": [],
   "source": [
    "class Buffer_Filler():\n",
    "    def __init__(self, agent, buffer_size = 200, params = DEFAULT_PARAMS):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.agent = agent\n",
    "        self.ns = params['ns']\n",
    "        self.na = params['na']\n",
    "        \n",
    "    def init_empty(self):\n",
    "        states = np.zeros((self.buffer_size, self.ns))\n",
    "        actions = np.zeros((self.buffer_size, self.na))\n",
    "        dones = np.zeros((self.buffer_size, 1))\n",
    "        rewards = np.zeros((self.buffer_size, 1))\n",
    "        next_states = states.copy()\n",
    "        self.buffer = (states, actions, dones, rewards, next_states)\n",
    "    \n",
    "    def fill_buffer(self):\n",
    "        self.init_empty()\n",
    "        (states, actions, rewards, dones, next_states) = self.buffer\n",
    "        \n",
    "        i = 0\n",
    "        while i < self.buffer_size:\n",
    "            self.agent.playPol(explore = True)\n",
    "            n = len(self.agent.history)\n",
    "            for j in range(n):\n",
    "                s, a, r, d, nst = self.agent.history.popleft()\n",
    "                idx = i + j\n",
    "                if idx >= self.buffer_size:\n",
    "                    break\n",
    "                states[idx, :] = s; actions[idx, :] = a; dones[idx, :] = d \n",
    "                rewards[idx, :] = r; next_states[idx, :] = nst\n",
    "            i += n\n",
    "        self.buffer = (states, actions, rewards, dones, next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RR8ZxPiJE-kq"
   },
   "outputs": [],
   "source": [
    "bf = Buffer_Filler(dqn_guy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PS-RO0CWJfxz"
   },
   "outputs": [],
   "source": [
    "bf.fill_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "r3S9K6b6M6KB",
    "outputId": "6568a2a8-3908-40ae-abcd-ac6387634227"
   },
   "outputs": [],
   "source": [
    "bf.buffer[0][13], bf.buffer[1][13], bf.buffer[2][13], bf.buffer[3][13], bf.buffer[4][13], "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUhlPa9VLeTh"
   },
   "source": [
    "## Create a mini-batch of experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgeYgHdtXqSj"
   },
   "outputs": [],
   "source": [
    "class replay_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, states, actions, y):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.size = states.shape[0]        \n",
    "        assert self.size == actions.shape[0]\n",
    "        assert self.size == y.shape[0]\n",
    "        \n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.states[idx], self.actions[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDCG-CWJNwe4"
   },
   "outputs": [],
   "source": [
    "class Minibatcher(Buffer_Filler):\n",
    "    \n",
    "    ## Later: 10 batches of 128\n",
    "    \n",
    "    def __init__(self, agent, params = DEFAULT_PARAMS):\n",
    "        \n",
    "        self.bn = params['bn']\n",
    "        self.bs = params['bs']\n",
    "        super().__init__(agent, self.bn*self.bs, params)\n",
    "        \n",
    "        self.gamma = params['gamma']\n",
    "        self.eval_arch = copy.deepcopy(self.agent.arch)\n",
    "        self.eval_arch.eval()\n",
    "        \n",
    "        \n",
    "    def getX(self):\n",
    "        return (torch.tensor(self.buffer[0], dtype = torch.float32),\n",
    "                torch.tensor(self.buffer[1], dtype = torch.int64))\n",
    "        \n",
    "    def getY(self):\n",
    "        \n",
    "        __, __, rewards, dones, next_states = self.buffer\n",
    "        next_states = torch.tensor(next_states, dtype = torch.float32)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_values = self.eval_arch(next_states).max(1)[0]\n",
    "            \n",
    "        done_mask = torch.ByteTensor(dones)\n",
    "        next_values[done_mask.squeeze()] = 0.0\n",
    "        \n",
    "        rewards = torch.tensor(rewards, dtype = torch.float32).squeeze()\n",
    "        \n",
    "        expected_values = next_values.squeeze() * self.gamma + rewards\n",
    "        \n",
    "        return expected_values\n",
    "        \n",
    "    def createDS(self):\n",
    "        self.fill_buffer()\n",
    "        states, actions = self.getX()\n",
    "        y = self.getY()\n",
    "        self.ds = replay_Dataset(states, actions, y)\n",
    "        \n",
    "    def createDL(self):\n",
    "        self.createDS()\n",
    "        self.dl = DataLoader(self.ds, batch_size = self.bs, shuffle=True,\n",
    "                            num_workers = 4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TE1N1rHO5QF"
   },
   "outputs": [],
   "source": [
    "mb = Minibatcher(dqn_guy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RxQEpdnPBoE"
   },
   "outputs": [],
   "source": [
    "mb.createDL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-3VeBwfnXsQz",
    "outputId": "3c508d00-e3e0-42a9-b8f5-7f07c3569378"
   },
   "outputs": [],
   "source": [
    "len(mb.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "lf3mbGU0aN4X",
    "outputId": "d24e99f2-6e4b-4d1f-cd5d-a4d3bb2f8042"
   },
   "outputs": [],
   "source": [
    "mb.ds.__getitem__(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnSkwTUwgQFN"
   },
   "source": [
    "## Calculate Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWgPEPXtfDvR"
   },
   "outputs": [],
   "source": [
    "def loss_batch(states, actions, ys, model, opt=None):\n",
    "    \n",
    "    '''Calculates the loss for a minibatch, and (if opt is given) updates parameters'''\n",
    "    \n",
    "    loss_func = nn.MSELoss()\n",
    "    \n",
    "    expected = torch.gather(model(states), 1, actions).cuda()  \n",
    "    loss = loss_func(expected.squeeze(), ys.cuda())\n",
    "    \n",
    "    if opt is not None:  # Update parameters\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        \n",
    "    return loss.item(), len(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IIIGY5AhyXB"
   },
   "outputs": [],
   "source": [
    "states, actions, ys = next(iter(mb.dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HjRVe657-NWs",
    "outputId": "4276cb37-f04d-4e5d-ed1e-ccd77ca8b9b7"
   },
   "outputs": [],
   "source": [
    "loss_batch(states, actions, ys, arch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uroj-qXbYWj"
   },
   "source": [
    "## Create a trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1nfYBGC1I1r"
   },
   "outputs": [],
   "source": [
    "def run_avg(x, beta = 0.95, bias_correct = True):\n",
    "    '''Calculates exponential running average of x.'''\n",
    "    r = 0; res = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        r = beta*r + (1-beta)*x[i]\n",
    "        res[i] = r\n",
    "    if bias_correct:\n",
    "        res = res / (1 - beta **(np.arange(len(x))+1))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "388rSmqwbtUc"
   },
   "outputs": [],
   "source": [
    "class Learner(Minibatcher):\n",
    "    \n",
    "    '''This class trains our policy.'''\n",
    "    \n",
    "    def __init__(self, agent, params = DEFAULT_PARAMS):       \n",
    "        \n",
    "        super().__init__(agent, params = params)\n",
    "        self.epoch = 0  # The number of epochs for training.\n",
    "        self.rewards = []  # List of rewards      \n",
    "        self.losses = []\n",
    "        \n",
    "    def train(self, epochs, lr): \n",
    "        '''Trains our policy for epochs rounds with learning rate lr.'''\n",
    "        for epoch in tqdm(range(epochs), position = 0):\n",
    "            if (epoch) % 10 == 0:  # Update plot every 10 rounds\n",
    "                self.plot_reward()\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "            self.train_episode(lr)\n",
    "        plt.close('all')\n",
    "    \n",
    "    def train_episode(self, lr):\n",
    "        '''Trains our policy for a single epoch.'''\n",
    "        \n",
    "        self.epoch += 1      \n",
    "        self.createDL()   # Creates minibatch of replays to train on\n",
    "        \n",
    "        model = self.agent.arch\n",
    "        opt = optim.SGD(model.parameters(), lr=lr, momentum=0.95,\n",
    "                        weight_decay=1e-3)\n",
    "        \n",
    "        # Train Neural Net\n",
    "        model.train()\n",
    "        for sb, ab, yb in self.dl:\n",
    "            self.losses.append(loss_batch(sb, ab, yb, model, opt)[0])       \n",
    "        model.eval()\n",
    "        \n",
    "        # Play once; calculate and return  reward\n",
    "        rew, _ = self.agent.playPol(explore = True)\n",
    "        self.rewards.append(rew)\n",
    "        return rew\n",
    "        \n",
    "    def plot_reward(self):\n",
    "        '''Plot rewards.'''\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5),\n",
    "                                    sharex = False, sharey = False)\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.set_yscale('linear')\n",
    "        ax1.plot(run_avg(self.rewards, beta = 0.99), '-')\n",
    "        ax2.plot(run_avg(self.losses, beta = 0.999), '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1VIUL89v0_K1"
   },
   "outputs": [],
   "source": [
    "arch = nn.Sequential(nn.BatchNorm1d(8, affine = False),\n",
    "                    nn.Linear(8, 50), \n",
    "                    nn.LeakyReLU(inplace = True), \n",
    "                    nn.BatchNorm1d(50), \n",
    "                    nn.Linear(50, 4))     \n",
    "\n",
    "dqn_guy = DQAgent(ENV, arch)\n",
    "learn = Learner(dqn_guy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CFXpDqneYSRe",
    "outputId": "6698745a-4557-4d44-ad48-547edc5c070a"
   },
   "outputs": [],
   "source": [
    "np.mean([dqn_guy.playPol()[0] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "lBHAFYaN0l3k",
    "outputId": "4b76d572-f30a-450b-e359-cfff0ae4dec8"
   },
   "outputs": [],
   "source": [
    "learn.train(1, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "PM0wYEVWaCSx",
    "outputId": "61a59bdb-8d81-4175-eee3-80f34237db97"
   },
   "outputs": [],
   "source": [
    "learn.train(10, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4NGoeKZQaAO8",
    "outputId": "32ef1dcb-bd0a-4f32-b607-e38e99581124"
   },
   "outputs": [],
   "source": [
    "learn.agent.exp_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "TWC2-JuoaZiV",
    "outputId": "00a253fc-a08b-4e7d-c22f-1d63b5f87e75"
   },
   "outputs": [],
   "source": [
    "learn.train(100, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "04CFnd5b1F4w",
    "outputId": "0f81ccd3-feb9-4bd0-bf40-a1fa7ea33f65"
   },
   "outputs": [],
   "source": [
    "learn.agent.exp_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q1bP24PB1dWd",
    "outputId": "dd0f7bc9-dd78-4d07-a985-a9c2992bb452"
   },
   "outputs": [],
   "source": [
    "np.mean([dqn_guy.playPol()[0] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "Z08c5WmAcoTO",
    "outputId": "ffeb4dba-cc5f-4076-f11f-893aed8d0f59"
   },
   "outputs": [],
   "source": [
    "learn.train(100, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9rgeCkHt1gzr",
    "outputId": "8ae1e8f2-7347-4da5-8d39-510c9533dcea"
   },
   "outputs": [],
   "source": [
    "learn.agent.exp_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cAckpriN2oG5",
    "outputId": "23f03255-6369-4c98-a385-7faa802da472"
   },
   "outputs": [],
   "source": [
    "np.mean([dqn_guy.playPol()[0] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "fAy_ga6F2tVP",
    "outputId": "d4791ee4-c83b-4de0-b4db-ed64e305a4e1"
   },
   "outputs": [],
   "source": [
    "learn.train(100, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "y8rO_VSa7mTh",
    "outputId": "749bc430-0567-465b-b924-717202728ded"
   },
   "outputs": [],
   "source": [
    "learn.agent.exp_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VLOmVr9JvJXE",
    "outputId": "ed3e53c5-ec02-47c1-a0fb-d38c4b8778de"
   },
   "outputs": [],
   "source": [
    "np.mean([dqn_guy.playPol()[0] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "0vwV0Pl5vK75",
    "outputId": "dc458b17-bce7-4287-fab5-c2bc54e9008b"
   },
   "outputs": [],
   "source": [
    "learn.train(100, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Qa1uOH5lvNJK",
    "outputId": "2550b840-d9b6-4674-a8bc-ae32ba0b4708"
   },
   "outputs": [],
   "source": [
    "np.mean([dqn_guy.playPol()[0] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "9S342-aIvt2Y",
    "outputId": "1e3d6663-973d-4441-c004-9cc9a91fc0bc"
   },
   "outputs": [],
   "source": [
    "learn.train(200, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FbrFNHUowRX3",
    "outputId": "7e1757e6-eedf-4eed-e8e5-a788d8886eb3"
   },
   "outputs": [],
   "source": [
    "np.mean([dqn_guy.playPol()[0] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "BBaa9cQCwsND",
    "outputId": "c41c2930-aefa-437f-c3ce-45aede19e2ae"
   },
   "outputs": [],
   "source": [
    "learn.train(300, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2054
    },
    "colab_type": "code",
    "id": "7EgSXU-pxNnb",
    "outputId": "3198ed05-700e-45cc-f44b-c9f22c6cfc7c"
   },
   "outputs": [],
   "source": [
    "learn.train(300, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1dszihcy3Lp"
   },
   "outputs": [],
   "source": [
    "np.mean([dqn_guy.playPol()[0] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dt1xlfTFzQtw"
   },
   "outputs": [],
   "source": [
    "learn.train(500, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8oanL-D0anT"
   },
   "outputs": [],
   "source": [
    "learn.train(500, LR/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myTZYLLU1JjL"
   },
   "outputs": [],
   "source": [
    "np.mean([dqn_guy.playPol()[0] for i in range(100)])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "week6.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
